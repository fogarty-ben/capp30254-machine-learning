{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \\#2: Simple Machine Learning Pipeline\n",
    "\n",
    "Ben Fogarty  \n",
    "University of Chicago, Harris School of Public Policy  \n",
    "CAPP 30254: Machine Learning for Public Policy  \n",
    "Spring 2019\n",
    "\n",
    "## Project overview & requirements\n",
    "\n",
    "This project's folder contains the following files:\n",
    "\n",
    "- writeup.ipynb: the assignment write up\n",
    "- pipeline_library.py: general functions for a machine learning pipeline (reading data, preprocessing data, generating features, building models, etc.)\n",
    "- predict_financial.py: specific functions for applying the functions in pipeline_library to predicting who will experience financial distress within the next two years\n",
    "- tree.pdf: a visualization of the decision tree generated in this model\n",
    "- credit-data.csv: the dataset used for training and testing the tree predicting who will experience financial distress within the next two years\n",
    "- data-dictionary.csv: dictionary describing the dataset in credit-data.csv\n",
    "- hw2.pdf: the assignment statement\n",
    "\n",
    "The project was developed using Python 3.7.3 on MacOS Mojave 10.14.4. It requires the following libraries:\n",
    "\n",
    "| Package        | Version     |\n",
    "| :------------: | :---------: |\n",
    "| graphviz       | 2.40.1      |\n",
    "| pandas         | 0.24.2      |\n",
    "| matplotlib     | 3.0.3       |\n",
    "| numpy          | 1.16.2      |\n",
    "| seaborn        | 0.9.0       |\n",
    "| scikit-learn   | 0.20.3      |\n",
    "\n",
    "Helpful documentation and references are cited throughout the docstrings of the code.\n",
    "\n",
    "## Building a simple machine learning pipeline\n",
    "\n",
    "All code for this portion of the project is located in the pipeline_library module. Excerpts from this module are included throughout.\n",
    "\n",
    "### Read data\n",
    "\n",
    "The pipeline_library module provides a function, read_csv, which imports CSV files into pandas dataframes, optionally allowing for the user to specify which columns to import from the csv and what the type of the columns should be in the result dataframe. This function simply wraps the read_csv function provided by the pandas library."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def read_csv(filepath, cols=None, col_types=None):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    return pd.read_csv(filepath, usecols=cols, dtype=col_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data\n",
    "\n",
    "The pipeline_library module also provides a suite for functions for exploratory data analysis. The first, show_distribution, returns a histogram and box plot for variable with a numeric type, and a bar plot for variables with a non-numeric type."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def show_distribution(df, variable):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    sns.set()\n",
    "    if pd.api.types.is_numeric_dtype(df[variable]):\n",
    "        f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "        sns.distplot(df[variable], kde=False, ax=ax1)\n",
    "        sns.boxplot(x=variable, data=df, ax=ax2, orient='h')\n",
    "        ax1.set_title('Histogram')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax2.set_title('Box plot')\n",
    "    else:\n",
    "        f, ax = plt.subplots(1, 1)\n",
    "        val_counts = df[variable].value_counts()\n",
    "        sns.barplot(x=val_counts.index, y=val_counts.values, ax=ax)\n",
    "        ax.set_ylabel('Count')\n",
    "\n",
    "    f.suptitle('Distribution of {}'.format(variable))\n",
    "    f.subplots_adjust(hspace=.5, wspace=.5)\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next, pw_correlate, calculates a table of pairwise correlations between numeric type variables. The user can optionally specify which variables to include pairwise correlations for, and enable visualization. If visualization is enabled, the function also generates a heat map to help the user identify strong correlations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def pw_correlate(df, variables=None, visualize=False):\n",
    "    '''\n",
    "    ..\n",
    "    '''\n",
    "    if not variables:\n",
    "        variables = [col for col in df.columns\n",
    "                         if pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "    corr_table = np.corrcoef(df[variables].dropna(), rowvar=False)\n",
    "    corr_table = pd.DataFrame(corr_table, index=variables, \n",
    "                              columns=variables)\n",
    "\n",
    "    if visualize:\n",
    "        sns.set()\n",
    "        f, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.heatmap(corr_table, annot=True, fmt='.2f',      \n",
    "                    linewidths=0.5, vmin=0, vmax=1, square=True,  \n",
    "                    cmap='coolwarm', ax=ax)\n",
    "\n",
    "        labels = ['-\\n'.join(wrap(l.get_text(), 15)) for l in \n",
    "                  ax.get_yticklabels()]\n",
    "        ax.set_yticklabels(labels)\n",
    "        labels = ['-\\n'.join(wrap(l.get_text(), 15)) for l in \n",
    "                  ax.get_xticklabels()]\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.tick_params(axis='both', rotation=0, labelsize='small')\n",
    "        ax.tick_params(axis='x', rotation=90, labelsize='small')\n",
    "\n",
    "        f.suptitle('Correlation Table')\n",
    "        f.tight_layout()\n",
    "        f.show()\n",
    "\n",
    "    return corr_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function summarize_data provides summary statistics over numeric data columns. By default, the function summarizes over all numeric columns, however, the user can restrict the summary statistics to certain numeric columns using the agg_cols positional keyword. Additonally, the user can also change the aggregating functions; the default are mean, variance, and quartiles. Lastly, the user can also chose to group observations based on one or more categorical variables and then compute summaries over each group. This functionality can be helpful for seeing the relationship between categorical variables and other numeric type variables."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def summarize_data(df, grouping_vars=None, agg_cols=None, \n",
    " agg_funcs=None):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    if not grouping_vars:\n",
    "        grouping_vars = []\n",
    "    if not agg_cols:\n",
    "        agg_cols = [col for col in df.columns\n",
    "                        if (pd.api.types.is_numeric_dtype(df[col]) \n",
    "                        and col not in grouping_vars)]\n",
    "    if not agg_funcs:\n",
    "        agg_funcs = [np.mean, np.var, \n",
    "                     lambda x: np.percentile(x, [.25, .5, .75])]\n",
    "    \n",
    "    if grouping_vars:\n",
    "        summary = df.groupby(grouping_vars)\\\n",
    "                    [agg_cols]\\\n",
    "                    .agg(agg_funcs)\n",
    "    else:\n",
    "        summary = df[agg_cols]\\\n",
    "                    .agg(agg_funcs)\n",
    "\n",
    "    return summary.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final function for exploratory data analysis, find_outliers, relies on a helper function find_outlier_univariate. The find_outliers function identifies the outliers in each numeric column of a dataframe, then records the number and percent of evaluated columns for which an observation is an outlier. The return values is a dataframe that links each row contains booleans describing whether the associated row in the passed in dataframe is considered an outlier for each numeric column and the numer and percent of evaulated columns for which the associated row is considered an outlier. For the pruposes of this analysis, an outlier is falling more than 1.5x the interquartile range below the 25th percentile value or more than 1.5x the interquartile range above the 75th percentile value. Optionally, the user can exclude certain columns from this procedure with the keyword argument excluded."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def find_oulier_univariate(series, visualize=False):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    quartiles = np.percentile(series.dropna(), [0.25, 0.75])\n",
    "    iqr = quartiles[1] - quartiles[0]\n",
    "    lower_bound = quartiles[0] - iqr\n",
    "    upper_bound = quartiles[1] + iqr\n",
    "\n",
    "    return (lower_bound > series) | (upper_bound < series)\n",
    "\n",
    "def find_outliers(df, excluded=None):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    if not excluded:\n",
    "        excluded = []\n",
    "\n",
    "    numeric_cols = list(df.select_dtypes(include=\n",
    "                        [np.number]).columns)\n",
    "\n",
    "    outliers = df[numeric_cols]\\\n",
    "                 .drop(excluded, axis=1, errors='ignore')\\\n",
    "                 .apply(find_oulier_univariate, axis=0)\n",
    "    outliers['Count Outlier'] = outliers.sum(axis=1, \n",
    "                                             numeric_only=True)\n",
    "    outliers['% Outlier'] = (outliers['Count Outlier'] /\n",
    "                             (len(outliers.columns) - 1) * 100)\n",
    "\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "\n",
    "The preprocess_data function in the pipeline_library module also relies on a helper function, replace_missing. At this time, the only preprocessing step is to replace missing vaules for any variables that have missing values. The replace_missing function take one column of a dataframe in the form of a series as its input. It then determines wheter the series contains numeric type data, and if so, replaces the missing values with the median value in the series. If the series does not contain numeric type data, the data is assumed to be unordered categorical data, and the functions replaces missing values with the modal value in the series, since a median cannot be calculated for unordered categorical data. The preprocess_data functions applies this algorithm for replacing missing data to all the columns of a given dataframe."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def replace_missing(series):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        median = np.median(series.dropna())\n",
    "        return series.fillna(median)\n",
    "    else:\n",
    "        mode = series.mode().iloc[0]\n",
    "        return series.fillna(mode)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    return df.apply(replace_missing, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features/predictors\n",
    "\n",
    "To discretize a continuous variable, the pipeline_library module provies the cut_variable function. This functions takes in a single columns of a dataframe (in the form of a pandas series) and returns that column discretized into bins. The user can either specify a list of \"edges\" for the bins (for example \\[0, 0.5, 1.0\\] would create the bins \\[0, 0.5) and \\[0.5, 1) ) or a number of bins, which creates n approximately equipercentile bins. The user can also specify labels for the bins.\n",
    "\n",
    "Though the pandas library contains a function, pd.cut, which can be used to discretize continuous data, I decided to write a custom function for greater control over choosing bin size and greater certainty about how continuous variables are being discretized."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cut_variable(series, bins, labels=None):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    if type(bins) is int:\n",
    "        min_val = min(series)\n",
    "        max_val = max(series)\n",
    "        range_size = max_val - min_val\n",
    "        max_val = max_val + range_size * 0.001\n",
    "        precentiles = np.linspace(0, 1, num=(bins + 1))\n",
    "\n",
    "        bins = np.percentile(series, percentiles).tolist()\n",
    "        bins[-1] = max_val\n",
    "\n",
    "    cut = pd.Series(index=series.index)\n",
    "    if labels:\n",
    "        assert len(labels) == (len(bins) - 1), ('You must specify the same ' +\n",
    "                                                'number of labels and bins.')\n",
    "\n",
    "    for i in range(len(bins) - 1):\n",
    "        lb = bins[i]\n",
    "        ub = bins[i + 1]\n",
    "        if labels:\n",
    "            cut[(lb <= series) & (series < ub)] = labels[i]\n",
    "        else:\n",
    "            cut[(lb <= series) & (series < ub)] = \"[{0:.3f} to {1:.3f})\".format(lb,\n",
    "            ub)\n",
    "\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert a categorical varaible into a set of dummy variables, the pipeline_library module also provides another custome function, create_dummies. This function takes in a dataframe and the name of the column to create dummies from, and returns a new dataframe with the categorical column remove and the new dummy columns appended to the end of the dataframe. \n",
    "\n",
    "The pandas library also provides a function to convert categorical varaibles to dummies, pd.get_dummies. I chose to write a custom function, however, because I was dissatisfied with how the pandas library encodes missing data (it makes all dummies false where the categorical column is NA, whereas the function in pipeline_library makes dummies with missing values where the categorical column is NA) and to provide the opportunity for additional customization in the future."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_dummies(df, column):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    col = df[column]\n",
    "    values = list(col.value_counts().index)\n",
    "    output = df.drop(column, axis=1)\n",
    "    for value in values:\n",
    "        dummy_name = '{}_{}'.format(column, value)\n",
    "        output[dummy_name] = (col == value)\n",
    "        output.loc[col.isnull(), dummy_name] = float('nan')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier\n",
    "\n",
    "The pipeline designed for this project can be used to generate a Decision Tree Classifier using the generate_decision_tree function. The function takes in training data in the form of a pandas dataframe of features, and pandas series of labels for the same observations, and optionally an instance of sklearn.tree.DecisionTreeClassifier. By default, the decision tree generated by this function uses all the default values specified in the sklearn.tree.DecisionTreeClassifier object ([see the documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)), except it uses information gain instead of Gini impurity as its criterion for splitting. The optional decision tree parameter allows for the user to customize the properties of the DecisionTreeClassifier by instantiating a sklearn.tree.DecisionTreeClassifier and passing it to the function. If the user passes a DecisionTreeClassifier object, that object is used instead of the default decision tree generate by the function. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_decision_tree(features, target, dt=None):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    if not dt:\n",
    "        dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "    return dt.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifier\n",
    "\n",
    "A simple function, score_decision_tree, returns the mean accuracy of a decision tree when used to predict the target attribute for a set of observations where the value of the target attribute is known. The function takes in a decision tree, and a set of testing data in the form of a pandas dataframe of features (in the same order as the data on which the tree was trained) and a pandas series of classes for the same observations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def score_decision_tree(dt, test_features, test_target):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    return dt.score(test_features, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Classifier\n",
    "\n",
    "Lastly, the function visualize_decision_tree saves and opens a PDF containing a visual representation of a DecisionTreeClassifer. For this function, the user must provide a decision tree, a list of feature names in the same order as the data on which the tree was trained, and the list of class names for the target attribute that the tree predicts. Optionally, the user may also specify an output path for the generate PDF."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def visualize_decision_tree(dt, feature_names, class_names, filepath='tree'):\n",
    "    '''\n",
    "    ...\n",
    "    '''\n",
    "    class_names.sort()\n",
    "    dot_data = tree.export_graphviz(dt, None, feature_names=feature_names, \n",
    "                                  class_names=class_names, filled=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    output_path = graph.render(filename=filepath, view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the simple machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
